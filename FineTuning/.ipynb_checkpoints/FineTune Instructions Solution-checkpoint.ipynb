{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solution_intro",
   "metadata": {},
   "source": [
    "# Fine-Tune GPT-2 for Instruction Following - Solution\n",
    "\n",
    "This notebook demonstrates how to fine-tune a GPT-2 model on an instruction-following dataset. Fine-tuning adapts a pre-trained language model to perform specific tasks based on instructions, making it more useful for downstream applications like chatbots or text generation based on prompts.\n",
    "\n",
    "We will use the `transformers` and `datasets` libraries from Hugging Face, which provide convenient tools for this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_info",
   "metadata": {},
   "source": [
    "## Dataset: `hakurei/open-instruct-v1`\n",
    "\n",
    "For this fine-tuning task, we will use the `hakurei/open-instruct-v1` dataset from the Hugging Face Hub. This dataset is designed for instruction tuning and has a specific structure, which is important to understand for preprocessing.\n",
    "\n",
    "You can find the dataset card with more details here: [https://huggingface.co/datasets/hakurei/open-instruct-v1](https://huggingface.co/datasets/hakurei/open-instruct-v1)\n",
    "\n",
    "Key characteristics of this dataset:\n",
    "\n",
    "-   It contains pairs of instructions, optional inputs, and corresponding outputs.\n",
    "-   The relevant columns for our task are `instruction`, `input`, and `output`.\n",
    "-   We will need to combine these columns into a single text format that the GPT-2 model can learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, we need to install the necessary libraries: `datasets`, `transformers`, and `torch`. `torch` is the deep learning framework that `transformers` uses under the hood for model computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install datasets for loading and processing the dataset\n",
    "# Install transformers for the model, tokenizer, and training utilities\n",
    "# Install torch as the backend for transformers\n",
    "!pip install datasets transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_model_tokenizer",
   "metadata": {},
   "source": [
    "## 2. Load Dataset, Model, and Tokenizer\n",
    "\n",
    "We will load the `hakurei/open-instruct-v1` dataset, the GPT-2 tokenizer, and the pre-trained GPT-2 model. We'll use a small subset of the dataset for faster training during this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load a small subset of the dataset for demonstration purposes\n",
    "# We use 'train[:1%]' to load only the first 1% of the training data\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"hakurei/open-instruct-v1\", split=\"train[:1%]\")\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "# The tokenizer converts text into token IDs that the model understands\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# Set the padding token to the EOS token, as GPT-2 doesn't have a dedicated pad token by default\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Load the pre-trained GPT-2 model for causal language modeling\n",
    "# This model will be fine-tuned on our instruction dataset\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization",
   "metadata": {},
   "source": [
    "## 3. Tokenization and Data Formatting\n",
    "\n",
    "Language models work with numerical tokens, not raw text. We need to tokenize our dataset and format it into sequences that the model can learn from. For instruction tuning, a common approach is to concatenate the `instruction`, `input`, and `output` fields into a single text string, often using special separators or formatting to distinguish the different parts.\n",
    "\n",
    "We will create a `tokenize` function that takes a batch of examples, formats the text, and then tokenizes it. We'll use `tokenizer.eos_token` at the end of each combined example to signal the end of a sequence. The `datasets` library's `.map()` method is used to apply this function efficiently across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize(batch):\n",
    "    # This function takes a batch of examples from the dataset.\n",
    "    # We iterate through each example in the batch.\n",
    "    texts = []\n",
    "    for instruction, input_text, output_text in zip(batch['instruction'], batch['input'], batch['output']):\n",
    "        # Format the text for instruction tuning.\n",
    "        # We use a simple format: Instruction: ...\\nInput: ...\\nOutput: ...\n",
    "        # The {tokenizer.eos_token} is added at the end to mark the end of the sequence.\n",
    "        if input_text: # Check if there is an input field\n",
    "            text = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput: {output_text}{tokenizer.eos_token}\"\n",
    "        else: # Handle cases with no input field\n",
    "            text = f\"Instruction: {instruction}\\nOutput: {output_text}{tokenizer.eos_token}\"\n",
    "        texts.append(text)\n",
    "\n",
    "    # Tokenize the list of combined text strings.\n",
    "    # truncation=True cuts off texts longer than max_length.\n",
    "    # padding=\"max_length\" pads shorter texts to max_length.\n",
    "    # max_length=64 sets the maximum sequence length.\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "# batched=True processes data in batches, which is faster.\n",
    "# remove_columns removes the original text columns after tokenization to save memory.\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"instruction\", \"input\", \"output\"])\n",
    "print(\"Dataset tokenized and formatted.\")\n",
    "\n",
    "# A Data Collator is needed to prepare batches during training.\n",
    "# DataCollatorForLanguageModeling handles the shifting of labels for causal language modeling (predicting the next token).\n",
    "# mlm=False specifies that we are doing causal language modeling, not masked language modeling.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_setup",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "We use the `Trainer` class from `transformers` to handle the training loop. We need to define `TrainingArguments` which specify hyperparameters and training configurations like output directory, batch size, learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-instruct\",  # Directory to save checkpoints and results\n",
    "    per_device_train_batch_size=4, # Batch size per device (adjust based on your GPU memory)\n",
    "    num_train_epochs=1,            # Number of training epochs\n",
    "    logging_steps=10,              # Log training loss every X steps\n",
    "    save_steps=100,                # Save a checkpoint every X steps\n",
    "    fp16=True,                     # Enable mixed precision training (recommended for GPUs)\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "# The Trainer brings together the model, training arguments, dataset, and data collator\n",
    "trainer = Trainer(\n",
    "    model=model,                  # The model to train\n",
    "    args=args,                    # The training arguments\n",
    "    train_dataset=tokenized_dataset, # The tokenized training dataset\n",
    "    data_collator=data_collator,  # The data collator\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "Now we can start the training process by calling the `trainer.train()` method. This will run the fine-tuning for the specified number of epochs, logging progress and saving checkpoints as configured in `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training loop\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model",
   "metadata": {},
   "source": [
    "## 6. Save the Fine-Tuned Model and Tokenizer\n",
    "\n",
    "The `Trainer` automatically saves checkpoints during training based on `save_steps`. It also saves the final model and tokenizer in the `output_dir` after training completes. However, you can explicitly save the tokenizer as well to be certain all necessary files are in the output directory for later loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit_save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly save the tokenizer to the output directory\n",
    "# This ensures all tokenizer files are present for loading later\n",
    "tokenizer.save_pretrained(args.output_dir)\n",
    "print(f\"Model and tokenizer saved to {args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_inference",
   "metadata": {},
   "source": [
    "## 7. Load and Use the Fine-Tuned Model for Inference\n",
    "\n",
    "After training, you can load your fine-tuned model and tokenizer from the `output_dir` and use it for text generation. The `transformers` pipeline is a convenient way to do this. Remember to format your input prompt for inference in the same way you formatted the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the directory where your trained model was saved (probably need to access the latest checkpoint folder)\n",
    "output_dir = \"./gpt2-instruct/checkpoint-1247/\"\n",
    "\n",
    "# Load the tokenizer and model from the saved directory\n",
    "print(f\"Loading model and tokenizer from {output_dir} for inference...\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "\n",
    "# Create a text generation pipeline using your fine-tuned model and tokenizer\n",
    "generator = pipeline(\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "\n",
    "# Now you can use the generator pipeline to generate text\n",
    "# Format your prompt according to the structure used during fine-tuning\n",
    "prompt = \"Instruction: Write a short poem about nature.\\nOutput:\"\n",
    "\n",
    "# Generate text\n",
    "# Adjust max_length and other generation parameters as needed\n",
    "print(f\"Generating text with prompt:\\n{prompt}\")\n",
    "generated_output = generator(prompt, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "print(f\"\\nGenerated Text:\\n{generated_output}\")\n",
    "\n",
    "# Post-process the output to get just the generated response (optional)\n",
    "if generated_output.startswith(prompt):\n",
    "    generated_response = generated_output[len(prompt):].strip()\n",
    "    print(f\"\\nGenerated Response (post-processed):\\n{generated_response}\")\n",
    "else:\n",
    "     print(\"Could not automatically remove prompt from output. Full generated text shown above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
