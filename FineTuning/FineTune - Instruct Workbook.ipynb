{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "workbook_intro",
   "metadata": {},
   "source": [
    "# Fine-Tune GPT-2 for Instruction Following - Workbook\n",
    "\n",
    "This workbook will guide you through the process of fine-tuning a GPT-2 model on an instruction-following dataset. Fine-tuning adapts a pre-trained language model to perform specific tasks based on instructions.\n",
    "\n",
    "You will use the `transformers` and `datasets` libraries from Hugging Face to:\n",
    "\n",
    "1.  Install necessary libraries.\n",
    "2.  Load the dataset, model, and tokenizer.\n",
    "3.  Process and tokenize the data for training.\n",
    "4.  Set up and run the training process.\n",
    "5.  Save the fine-tuned model.\n",
    "6.  Load and use the fine-tuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_info_workbook",
   "metadata": {},
   "source": [
    "## Dataset: `hakurei/open-instruct-v1`\n",
    "\n",
    "We will use the `hakurei/open-instruct-v1` dataset for instruction tuning. Understanding its structure is key to preparing the data correctly.\n",
    "\n",
    "Explore the dataset card here: [https://huggingface.co/datasets/hakurei/open-instruct-v1](https://huggingface.co/datasets/hakurei/open-instruct-v1)\n",
    "\n",
    "Note the `instruction`, `input`, and `output` columns. You will need to combine these for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation_workbook",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install the required libraries: `datasets`, `transformers`, and `torch`. These provide the tools for data handling, model management, and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_libs_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install datasets, transformers, and torch using pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_model_tokenizer_workbook",
   "metadata": {},
   "source": [
    "## 2. Load Dataset, Model, and Tokenizer\n",
    "\n",
    "Load the `hakurei/open-instruct-v1` dataset (use a small split like `train[:1%]` for speed), the GPT-2 tokenizer, and the pre-trained `gpt2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_assets_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# TODO: Load a small subset of the 'hakurei/open-instruct-v1' dataset (e.g., train[:1%])\n",
    "print(\"Loading dataset...\")\n",
    "dataset = None # Replace with your code\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "# TODO: Load the GPT-2 tokenizer from 'gpt2'\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = None # Replace with your code\n",
    "# TODO: Set the tokenizer's pad_token to its eos_token\n",
    "\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# TODO: Load the pre-trained GPT-2 model for causal language modeling from 'gpt2'\n",
    "print(\"Loading model...\")\n",
    "model = None # Replace with your code\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization_workbook",
   "metadata": {},
   "source": [
    "## 3. Tokenization and Data Formatting\n",
    "\n",
    "Create a function to tokenize the dataset. This function should combine the `instruction`, `input`, and `output` columns into a single text string for each example, adding the `tokenizer.eos_token` at the end. Use a format like `Instruction: ...\\nInput: ...\\nOutput: ...`.\n",
    "\n",
    "Apply this function to the dataset using `.map()`, ensuring you process in batches (`batched=True`) and remove the original text columns (`remove_columns`). Define a `DataCollatorForLanguageModeling` for preparing training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize_data_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# TODO: Define the tokenize function that combines 'instruction', 'input', and 'output'\n",
    "#       and tokenizes the result. Remember to add eos_token and handle padding/truncation.\n",
    "def tokenize(batch):\n",
    "    pass # Replace with your code\n",
    "\n",
    "# TODO: Apply the tokenization function to the dataset using .map()\n",
    "#       Ensure batched=True and remove the original columns.\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = None # Replace with your code\n",
    "print(\"Dataset tokenized and formatted.\")\n",
    "\n",
    "# TODO: Define a DataCollatorForLanguageModeling with mlm=False\n",
    "data_collator = None # Replace with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_setup_workbook",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "Define the `TrainingArguments` for your fine-tuning process. Specify the `output_dir`, `per_device_train_batch_size`, `num_train_epochs`, `logging_steps`, `save_steps`, and enable `fp16` for GPU acceleration.\n",
    "\n",
    "Then, initialize the `Trainer` with your model, arguments, tokenized dataset, and data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_trainer_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# TODO: Define the TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    # Specify output directory, batch size, epochs, logging, saving, fp16, and dataloader_num_workers\n",
    "    pass # Replace with your code\n",
    ")\n",
    "\n",
    "# TODO: Initialize the Trainer\n",
    "trainer = None # Replace with your code\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_workbook",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "Start the training process by calling the `trainer.train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_training_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Start the training loop\n",
    "print(\"Starting training...\")\n",
    "pass # Replace with your code\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model_workbook",
   "metadata": {},
   "source": [
    "## 6. Save the Fine-Tuned Model and Tokenizer\n",
    "\n",
    "Although the Trainer saves checkpoints, explicitly save the tokenizer to ensure all necessary files are in your output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit_save_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explicitly save the tokenizer to the output directory (args.output_dir)\n",
    "pass # Replace with your code\n",
    "print(f\"Model and tokenizer saved to {args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_inference_workbook",
   "metadata": {},
   "source": [
    "## 7. Load and Use the Fine-Tuned Model for Inference\n",
    "\n",
    "Load your fine-tuned model and tokenizer from the `output_dir`. Create a `transformers` pipeline for text generation. Format your input prompt for inference consistently with your training data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# TODO: Define the output directory (include checkpoint-xxxx in the path)\n",
    "output_dir = None # Replace with your code\n",
    "\n",
    "# TODO: Load the tokenizer and model from the output directory\n",
    "print(f\"Loading model and tokenizer from {output_dir} for inference...\")\n",
    "loaded_tokenizer = None # Replace with your code\n",
    "loaded_model = None # Replace with your code\n",
    "\n",
    "# TODO: Create a text generation pipeline using the loaded model and tokenizer. Specify the device.\n",
    "generator = None # Replace with your code\n",
    "\n",
    "# TODO: Define an input prompt formatted like your training data (e.g., Instruction: ...\\nOutput:)\n",
    "prompt = None # Replace with your code\n",
    "\n",
    "# TODO: Generate text using the pipeline. Adjust max_length as needed.\n",
    "print(f\"Generating text with prompt:\\n{prompt}\")\n",
    "generated_output = None # Replace with your code\n",
    "\n",
    "print(f\"\\nGenerated Text:\\n{generated_output}\")\n",
    "\n",
    "# Optional: Post-process the output to remove the prompt part\n",
    "# TODO: Implement optional post-processing to get just the generated response\n",
    "pass # Replace with your code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
