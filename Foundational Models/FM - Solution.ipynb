{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_solution",
   "metadata": {},
   "source": [
    "# Loading LLM Clients - Solution\n",
    "\n",
    "This notebook demonstrates the complete steps for loading a local foundation model (GPT-2) using the Hugging Face `transformers` library and setting up a client to interact with the OpenAI API. It also includes examples of how to run inference (generate text) using both clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation_solution",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "We install the necessary libraries: `transformers` and `openai`. `torch` is included as it's a common backend for `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "install_libs_solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (1.79.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.11/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n"
     ]
    }
   ],
   "source": [
    "# Install the transformers and openai libraries\n",
    "!pip install transformers openai torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_gpt2_solution",
   "metadata": {},
   "source": [
    "## 2. Load a Foundation Model (GPT-2) with Transformers\n",
    "\n",
    "We use the `pipeline` function from `transformers` to easily load the pre-trained `gpt2` model for text generation. The `pipeline` handles the complexities of loading the model and its corresponding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_gpt2_model_solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 text generation pipeline loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import the pipeline function from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the 'text-generation' pipeline using the 'gpt2' model.\n",
    "# This will download the model weights and tokenizer files the first time it's run.\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "print(\"GPT-2 text generation pipeline loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_openai_solution",
   "metadata": {},
   "source": [
    "## 3. Setup the OpenAI API Client\n",
    "\n",
    "To use models hosted by OpenAI (like GPT-4, GPT-3.5, etc.), you need to use their official Python client library and provide your API key. It is **highly recommended** to load your API key from an environment variable (`OPENAI_API_KEY`) for security reasons.\n",
    "\n",
    "The `openai.OpenAI()` client automatically looks for the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup_openai_client_solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n",
      "OpenAI API client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import the main OpenAI client class\n",
    "from openai import OpenAI\n",
    "\n",
    "# Ensure your OPENAI_API_KEY environment variable is set before running this cell.\n",
    "# You can set it in your shell/terminal or in a .env file.\n",
    "# If absolutely necessary for testing in a notebook without environment variables,\n",
    "# you could uncomment and set it like this (replace with your actual key):\n",
    "%env OPENAI_API_KEY=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\n",
    "\n",
    "# Initialize the OpenAI client.\n",
    "# It automatically uses the OPENAI_API_KEY environment variable if found.\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    print(\"OpenAI API client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    print(\"Please ensure your OPENAI_API_KEY environment variable is set correctly.\")\n",
    "    client = None # Set client to None if initialization fails to prevent errors later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_gpt2_solution",
   "metadata": {},
   "source": [
    "## 4. Run Inference with GPT-2\n",
    "\n",
    "Now that the GPT-2 pipeline is loaded, we can use it to generate text by providing a prompt. The `generator` object abstracts away the details of tokenization, model execution, and decoding the output back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "run_gpt2_inference_solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating text with GPT-2 ---\n",
      "Prompt: Artificial intelligence is transforming the world by\n",
      "\n",
      "Generated Text:\n",
      "Artificial intelligence is transforming the world by accelerating the evolution of computation. In some ways, AI is the only artificial intelligence that can run our world. In other ways, it is the best intelligence technology that scientists have ever created. We don't see a lot of research on artificial intelligence until late in\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt for the GPT-2 model\n",
    "gpt2_prompt = \"Artificial intelligence is transforming the world by\"\n",
    "\n",
    "# Generate text using the GPT-2 pipeline\n",
    "# max_length controls the maximum number of tokens in the output (including the prompt)\n",
    "# num_return_sequences specifies how many different generated texts to get\n",
    "print(f\"\\n--- Generating text with GPT-2 ---\")\n",
    "print(f\"Prompt: {gpt2_prompt}\")\n",
    "\n",
    "try:\n",
    "    gpt2_output = generator(gpt2_prompt, max_length=60, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    print(gpt2_output)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during GPT-2 inference: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_openai_solution",
   "metadata": {},
   "source": [
    "## 5. Run Inference with OpenAI API\n",
    "\n",
    "If your OpenAI client was initialized successfully, you can use it to interact with OpenAI's models. We'll demonstrate a simple chat completion request using a model like `gpt-3.5-turbo`. Remember that using the OpenAI API incurs costs based on usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "run_openai_inference_solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating text with OpenAI API ---\n",
      "Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Tell me a fun fact about the ocean.'}]\n",
      "\n",
      "Generated Text:\n",
      "A fun fact about the ocean is that it covers about 70% of the Earth's surface, but more than 80% of the ocean remains unexplored and unmapped. There are still many mysteries waiting to be discovered in the depths of the ocean!\n"
     ]
    }
   ],
   "source": [
    "# Check if the OpenAI client was initialized successfully before attempting to use it\n",
    "if client:\n",
    "    try:\n",
    "        # Define the conversation messages for the chat completion.\n",
    "        # This is a list of dictionaries, each with a 'role' (system, user, assistant) and 'content'.\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a fun fact about the ocean.\"}\n",
    "        ]\n",
    "\n",
    "        # Create a chat completion request using the client.\n",
    "        # Specify the model you want to use and the list of messages.\n",
    "        print(\"\\n--- Generating text with OpenAI API ---\")\n",
    "        print(f\"Messages: {messages}\")\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", # You can change this to other models like \"gpt-4o-mini\"\n",
    "            messages=messages,\n",
    "            max_tokens=100 # Limit the response length to control cost and verbosity\n",
    "        )\n",
    "\n",
    "        # Extract the generated content from the response object\n",
    "        openai_output = completion.choices[0].message.content\n",
    "\n",
    "        print(\"\\nGenerated Text:\")\n",
    "        print(openai_output)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError using OpenAI API: {e}\")\n",
    "        print(\"Please ensure your API key is valid, you have billing set up if needed, and the model name is correct.\")\n",
    "else:\n",
    "    print(\"\\nOpenAI client not initialized. Skipping OpenAI inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_solution",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully loaded a local GPT-2 model and set up the OpenAI API client, and demonstrated how to generate text using both. You can now integrate these into your applications to leverage different language models based on your needs (local vs. API, specific model capabilities)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
