{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_workbook",
   "metadata": {},
   "source": [
    "# Loading LLM Clients Workbook\n",
    "\n",
    "This workbook will guide you through the basic steps of loading a local foundation model using the Hugging Face `transformers` library and setting up a client to interact with the OpenAI API. You will also add code to perform text generation (inference) with both clients.\n",
    "\n",
    "By the end of this workbook, you will be able to:\n",
    "\n",
    "1.  Install the necessary libraries.\n",
    "2.  Load a pre-trained GPT-2 model using `transformers`.\n",
    "3.  Initialize the OpenAI API client.\n",
    "4.  Run text generation with the local GPT-2 model.\n",
    "5.  Run text generation with an OpenAI model via the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation_workbook",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "We need to install the `transformers` library (which includes `torch` as a dependency for model computations) and the `openai` Python client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_libs_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install the transformers and openai libraries using pip\n",
    "# The transformers library will also install torch or tensorflow as a dependency\n",
    "# depending on your environment, but torch is commonly used with GPT-2.\n",
    "!pip install transformers openai torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_gpt2_workbook",
   "metadata": {},
   "source": [
    "## 2. Load a Foundation Model (GPT-2) with Transformers\n",
    "\n",
    "The Hugging Face `transformers` library provides a simple way to load and use many pre-trained language models, including GPT-2. The `pipeline` function is a high-level abstraction that makes it easy to perform tasks like text generation without manually handling tokenizers and models separately.\n",
    "\n",
    "We will load the `gpt2` model for a text generation pipeline. Research the `transformers.pipeline` function and how to specify the task and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_gpt2_model_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the pipeline function from transformers\n",
    "\n",
    "# TODO: Load the 'text-generation' pipeline using the 'gpt2' model\n",
    "# This downloads and caches the model and tokenizer if you haven't used them before.\n",
    "generator = None # Replace with your code\n",
    "\n",
    "print(\"GPT-2 text generation pipeline loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_openai_workbook",
   "metadata": {},
   "source": [
    "## 3. Setup the OpenAI API Client\n",
    "\n",
    "To use models hosted by OpenAI (like GPT-4, GPT-3.5, etc.), you need to use their official Python client library and provide your API key. It is **highly recommended** to load your API key from an environment variable (`OPENAI_API_KEY`) for security reasons.\n",
    "\n",
    "Research how to set environment variables in your operating system or within a Jupyter notebook environment. Then, initialize the OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_openai_client_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=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\n",
    "\n",
    "# TODO: Import the OpenAI class from the openai library\n",
    "\n",
    "# TODO: Initialize the OpenAI client\n",
    "# The client automatically reads the OPENAI_API_KEY environment variable.\n",
    "client = None # Replace with your code\n",
    "\n",
    "print(\"OpenAI API client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_gpt2_workbook",
   "metadata": {},
   "source": [
    "## 4. Run Inference with GPT-2\n",
    "\n",
    "Use the `generator` pipeline you loaded to generate text. Define a simple prompt and call the generator, specifying parameters like `max_length` and `num_return_sequences`. Print the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_gpt2_inference_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a prompt for the GPT-2 model\n",
    "gpt2_prompt = None # Replace with your code\n",
    "\n",
    "# TODO: Generate text using the GPT-2 pipeline. Specify max_length and num_return_sequences.\n",
    "print(f\"\\n--- Generating text with GPT-2 ---\")\n",
    "print(f\"Prompt: {gpt2_prompt}\")\n",
    "\n",
    "try:\n",
    "    gpt2_output = None # Replace with your code\n",
    "\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    # TODO: Print the generated output from GPT-2\n",
    "    pass # Replace with your code\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during GPT-2 inference: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_openai_workbook",
   "metadata": {},
   "source": [
    "## 5. Run Inference with OpenAI API\n",
    "\n",
    "If your OpenAI client was initialized successfully, use it to make a chat completion request. Define a list of messages with roles (`system`, `user`) and content. Call `client.chat.completions.create`, specifying a model (like `gpt-3.5-turbo`) and your messages. Extract and print the generated content from the response.\n",
    "\n",
    "Remember that using the OpenAI API incurs costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_openai_inference_workbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the OpenAI client was initialized successfully\n",
    "if client:\n",
    "    try:\n",
    "        # TODO: Define the messages for the chat completion (list of dicts)\n",
    "        messages = None # Replace with your code\n",
    "\n",
    "        # TODO: Create a chat completion request using client.chat.completions.create\n",
    "        #       Specify the model and messages, and optionally max_tokens.\n",
    "        print(\"\\n--- Generating text with OpenAI API ---\")\n",
    "        print(f\"Messages: {messages}\")\n",
    "\n",
    "        completion = None # Replace with your code\n",
    "\n",
    "        # TODO: Extract and print the generated content from the completion object\n",
    "        print(\"\\nGenerated Text:\")\n",
    "        pass # Replace with your code\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError using OpenAI API: {e}\")\n",
    "        print(\"Please ensure your API key is valid and you have billing set up if needed.\")\n",
    "else:\n",
    "    print(\"\\nOpenAI client not initialized. Skipping OpenAI inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_workbook",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully loaded a local GPT-2 model and set up the OpenAI API client, and added code to generate text using both. You can now integrate these into your applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
