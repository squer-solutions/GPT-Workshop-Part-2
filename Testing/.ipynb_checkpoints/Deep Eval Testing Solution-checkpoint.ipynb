{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c4660e-6c79-456f-8165-d349141a2483",
   "metadata": {},
   "source": [
    "# Deep Eval Testing Workbook\n",
    "\n",
    "This Jupyter notebook is structured as a workbook to guide you through testing a text-generation model (GPT-2) using deep-eval. It covers:\n",
    "\n",
    "1. **Setup** – Installing and importing required libraries, initializing models and evaluators.\n",
    "2. **Prompt Variation** – Defining and evaluating a variety of prompts.\n",
    "3. **Output Structure Validation** – Verifying that outputs conform to expected formats (e.g., valid JSON).\n",
    "4. **Output Content Validation** – Checking for hallucinations and correctness on known vs. unknown facts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2197-39e2-4bbd-bec3-4da927dee760",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Lets install the nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23286333-0459-406a-84f4-f550bdfaf5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers deepeval torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a4131-bb27-4e19-87a9-d6b59cc09440",
   "metadata": {},
   "source": [
    "### Lets setup our LLM (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9667f61-ffb5-4b57-a8eb-f8940610c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    HallucinationMetric,\n",
    "    JsonCorrectnessMetric\n",
    ")\n",
    "from deepeval import evaluate\n",
    "\n",
    "# Initialize model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "\n",
    "def run_gpt2(input, answer_length=30):\n",
    "    return generator(input, max_length=answer_length)[0]['generated_text'].replace(input, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a32b6-72fb-4a2f-aa65-d3b262ddfeec",
   "metadata": {},
   "source": [
    "### In order to evaluate the responses, we need another LLM. Lets use the default OpenAI in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e8c49-9bc3-41ac-95af-c335dad5ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=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\n",
    "\n",
    "from deepeval.models import GPTModel\n",
    "\n",
    "gpt4mini = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "hallucination = HallucinationMetric(threshold=0.0, model=gpt4mini)\n",
    "relevancy = AnswerRelevancyMetric(threshold=0.5, model=gpt4mini)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98dab5-04b1-4ae0-8f8a-66a6ec523009",
   "metadata": {},
   "source": [
    "## 2. Prompt Variation\n",
    "\n",
    "We generate sequences for different prompts and check answer relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b64a0-f192-43d3-b35a-bfe266a7d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A summary of climate change: \",\n",
    "    \"In a single concise sentences, summarization of the key aspects of climate change:\\n\",\n",
    "    \"Causes and effects of climate change (1 sentence):\\n\",\n",
    "]\n",
    "\n",
    "expected_output = \"Climate change is driven by the accumulation of greenhouse gases, such as carbon dioxide, in the atmosphere where these gases trap heat, leading to global warming, rising sea levels, and extreme weather.\"\n",
    "\n",
    "# Create test cases\n",
    "variation_cases = []\n",
    "for p in prompts:\n",
    "    out = run_gpt2(p, 100)\n",
    "    variation_cases.append(\n",
    "        LLMTestCase(input=p, actual_output=out, expected_output=expected_output)\n",
    "    )\n",
    "\n",
    "# Evaluate with relevance metric\n",
    "variation_results = evaluate(variation_cases, [relevancy])\n",
    "print(variation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec8eca-1300-4803-9a8c-03195dc056de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Output Structure Validation\n",
    "\n",
    "Use the built-in JsonCorrectnessMetric to ensure valid JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921719d5-a42f-4dcf-bb83-3bcdeffedd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_prompt = \"A JSON object with fields 'text' and 'label': {\"\n",
    "structure_out = generator(structure_prompt, max_length=100)[0]['generated_text']\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Any, Dict\n",
    "\n",
    "class ExpectedJsonStructure(BaseModel):\n",
    "    text: str\n",
    "    metadata: Dict[str, Any] # Or simply 'Any' if metadata can be truly anything.\n",
    "                             # If metadata has a known structure, you can define another Pydantic model for it.\n",
    "\n",
    "structure_case = LLMTestCase(\n",
    "    input=structure_prompt,\n",
    "    actual_output=structure_out\n",
    ")\n",
    "\n",
    "json_metric = JsonCorrectnessMetric(expected_schema=ExpectedJsonStructure)\n",
    "structure_results = evaluate([structure_case], [json_metric])\n",
    "print(structure_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b0c6d-da7a-4f87-bd3f-372938109c13",
   "metadata": {},
   "source": [
    "## 4. Output Content Validation\n",
    "\n",
    "### 4.1 Known Facts\n",
    "\n",
    "Check against an expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06b187-e676-4021-beb5-af8fd0da50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_context = [\"France is a country located in Western Europe. The capital and largest city of France is Paris. Lyon is another major city in France. \"]\n",
    "fact_prompt = \"The capital of france is \"\n",
    "fact_input = factual_context[0] + fact_prompt\n",
    "\n",
    "fact_case = LLMTestCase(\n",
    "    input=fact_prompt,\n",
    "    actual_output=run_gpt2(fact_input, 100),\n",
    "    expected_output=\"Paris\",\n",
    "    context=fact_context\n",
    ")\n",
    "\n",
    "fact_results = evaluate([fact_case], [hallucination, relevancy])\n",
    "print(fact_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a773f17-c53e-4e44-9308-a988d149c4a8",
   "metadata": {},
   "source": [
    "### 4.2 Unknown Facts\n",
    "\n",
    "Ensure the model does not hallucinate about impossible questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07989bd2-a9c5-43d7-8614-45f70c30db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_prompt = \"The name of the person who won the Nobel Prize in Physics in 3025 is\"\n",
    "unknown_expected = \"This date is in the future, no way to tell\"\n",
    "\n",
    "unknown_case = LLMTestCase(\n",
    "    input=unknown_prompt,\n",
    "    actual_output=run_gpt2(unknown_prompt),\n",
    "    expected_output=unknown_expected,\n",
    "    context=[\"This date is in the future, no way to tell\"]\n",
    ")\n",
    "\n",
    "unknown_results = evaluate([unknown_case], [hallucination, relevancy])\n",
    "print(unknown_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abcc93b-d3da-4b85-bdbf-8dad72e51b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
