{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c4660e-6c79-456f-8165-d349141a2483",
   "metadata": {},
   "source": [
    "# Deep Eval Testing Workbook\n",
    "\n",
    "This Jupyter notebook is structured as a workbook to guide you through testing a text-generation model (GPT-2) using deep-eval. It covers:\n",
    "\n",
    "1. **Setup** – Installing and importing required libraries, initializing models and evaluators.\n",
    "2. **Prompt Variation** – Defining and evaluating a variety of prompts.\n",
    "3. **Output Structure Validation** – Verifying that outputs conform to expected formats (e.g., valid JSON).\n",
    "4. **Output Content Validation** – Checking for hallucinations and correctness on known vs. unknown facts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2197-39e2-4bbd-bec3-4da927dee760",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Let's begin by setting up our environment. We need to install the necessary libraries for deep-eval and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23286333-0459-406a-84f4-f550bdfaf5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Install the required libraries: transformers, deepeval, and torch\n",
    "# Use pip to install them in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a4131-bb27-4e19-87a9-d6b59cc09440",
   "metadata": {},
   "source": [
    "### Let's setup our LLM (GPT-2)\n",
    "\n",
    "We will use GPT-2 as the language model to test. We need to import the necessary classes from the `transformers` library and initialize the pipeline. Research how to set a seed for reproducibility in the `transformers` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9667f61-ffb5-4b57-a8eb-f8940610c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "# TODO: Import LLMTestCase and the necessary metrics (AnswerRelevancyMetric, HallucinationMetric, JsonCorrectnessMetric) from deepeval\n",
    "# TODO: Import the evaluate function from deepeval\n",
    "\n",
    "# Initialize model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "# TODO: Set the seed for reproducibility (e.g., 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a32b6-72fb-4a2f-aa65-d3b262ddfeec",
   "metadata": {},
   "source": [
    "### In order to evaluate the responses, we need another LLM. Let's use the default OpenAI in this case.\n",
    "\n",
    "Deep-eval uses another LLM to evaluate the output of the model being tested. We will use a GPT model for this. You will need to set your OpenAI API key as an environment variable. Research how to set environment variables in a Jupyter notebook.\n",
    "\n",
    "We will then initialize the GPT model and the `HallucinationMetric` and `AnswerRelevancyMetric` from deep-eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e8c49-9bc3-41ac-95af-c335dad5ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=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\n",
    "\n",
    "from deepeval.models import GPTModel\n",
    "\n",
    "gpt4mini = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# TODO: Initialize the HallucinationMetric with a threshold of 0.0 and the gpt4mini model\n",
    "# TODO: Initialize the AnswerRelevancyMetric with a threshold of 0.5 and the gpt4mini model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98dab5-04b1-4ae0-8f8a-66a6ec523009",
   "metadata": {},
   "source": [
    "## 2. Prompt Variation\n",
    "\n",
    "We will now explore how different prompts affect the generated output and evaluate the answer relevancy using deep-eval. \n",
    "\n",
    "Below are a few example prompts and an expected output. Your task is to:\n",
    "1. Generate output for each prompt using the GPT-2 model.\n",
    "2. Create `LLMTestCase` objects for each prompt, including the input, generated output, and the `expected_output`.\n",
    "3. Use the `evaluate` function with the `relevancy` metric to assess how relevant the generated answers are to the expected output.\n",
    "4. Print the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b64a0-f192-43d3-b35a-bfe266a7d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A summary of climate change: \",\n",
    "    \"In a single concise sentences, summarization of the key aspects of climate change:\\n\",\n",
    "    \"Causes and effects of climate change (1 sentence):\\n\",\n",
    "]\n",
    "\n",
    "expected_output = \"Climate change is driven by the accumulation of greenhouse gases, such as carbon dioxide, in the atmosphere where these gases trap heat, leading to global warming, rising sea levels, and extreme weather.\"\n",
    "\n",
    "# Create test cases\n",
    "variation_cases = []\n",
    "for p in prompts:\n",
    "    # TODO: Generate output for the current prompt using the generator model. Limit the output length.\n",
    "    out = None # Replace with your code\n",
    "    # TODO: Append an LLMTestCase to variation_cases with the prompt as input, the generated output, and the expected_output.\n",
    "    pass # Replace with your code\n",
    "\n",
    "# TODO: Evaluate the variation_cases using the relevancy metric\n",
    "# TODO: Print the variation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec8eca-1300-4803-9a8c-03195dc056de",
   "metadata": {},
   "source": [
    "## 3. Output Structure Validation\n",
    "\n",
    "It's often important to ensure that the model's output adheres to a specific structure, such as JSON. We can use the built-in `JsonCorrectnessMetric` in deep-eval to validate this.\n",
    "\n",
    "Your task is to:\n",
    "1. Define a Pydantic model that represents the expected JSON structure.\n",
    "2. Generate output from the GPT-2 model using a prompt that asks for JSON output.\n",
    "3. Create an `LLMTestCase` with the prompt as input and the generated output.\n",
    "4. Initialize the `JsonCorrectnessMetric` with your defined expected schema.\n",
    "5. Evaluate the test case using the `json_metric`.\n",
    "6. Print the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921719d5-a42f-4dcf-bb83-3bcdeffedd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any, Dict\n",
    "\n",
    "structure_prompt = \"A JSON object with fields 'text' and 'label': {\"\n",
    "# TODO: Generate output for the structure_prompt using the generator model. Limit the output length.\n",
    "structure_out = None # Replace with your code\n",
    "\n",
    "\n",
    "# TODO: Define a Pydantic model named ExpectedJsonStructure with fields 'text' (str) and 'label' (str).\n",
    "# Research Pydantic models for defining data structures.\n",
    "class ExpectedJsonStructure(BaseModel):\n",
    "    pass # Replace with your code\n",
    "\n",
    "structure_case = LLMTestCase(\n",
    "    input=structure_prompt,\n",
    "    actual_output=structure_out\n",
    ")\n",
    "\n",
    "# TODO: Initialize the JsonCorrectnessMetric with the expected_schema set to ExpectedJsonStructure.\n",
    "json_metric = None # Replace with your code\n",
    "# TODO: Evaluate the structure_case using the json_metric\n",
    "structure_results = None # Replace with your code\n",
    "# TODO: Print the structure_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b0c6d-da7a-4f87-bd3f-372938109c13",
   "metadata": {},
   "source": [
    "## 4. Output Content Validation\n",
    "\n",
    "Now, let's validate the content of the model's output, specifically focusing on factual correctness and the absence of hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e676-4021-beb5-af8fd0da50d4",
   "metadata": {},
   "source": [
    "### 4.1 Known Facts\n",
    "\n",
    "We will check if the model can correctly answer questions based on provided context. Your task is to:\n",
    "1. Define a context and a prompt related to a known fact.\n",
    "2. Generate output from the GPT-2 model using the context and prompt.\n",
    "3. Define the expected output for the known fact.\n",
    "4. Create an `LLMTestCase` including the input, generated output, expected output, and the context. Research how to include context in an `LLMTestCase`.\n",
    "5. Evaluate the test case using the `hallucination` and `relevancy` metrics.\n",
    "6. Print the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06b187-e676-4021-beb5-af8fd0da50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_context = [\"France is a country located in Western Europe. The capital and largest city of France is Paris. Lyon is another major city in France. \"]\n",
    "fact_prompt = \"The capital of france is \"\n",
    "fact_input = fact_context[0] + fact_prompt\n",
    "# TODO: Generate output for the fact_input using the generator model. Limit the output length.\n",
    "fact_out = None # Replace with your code\n",
    "\n",
    "fact_expected = \"Paris\"\n",
    "\n",
    "# TODO: Create an LLMTestCase with the fact_prompt as input, the generated output (removing the input part from the output), the fact_expected output, and the fact_context.\n",
    "fact_case = None # Replace with your code\n",
    "\n",
    "# TODO: Evaluate the fact_case using the hallucination and relevancy metrics.\n",
    "fact_results = None # Replace with your code\n",
    "# TODO: Print the fact_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a773f17-c53e-4e44-9308-a988d149c4a8",
   "metadata": {},
   "source": [
    "### 4.2 Unknown Facts\n",
    "\n",
    "Finally, we will test if the model hallucinates when asked about information it could not possibly know (e.g., future events). Your task is to:\n",
    "1. Define a prompt for an unknown fact.\n",
    "2. Generate output from the GPT-2 model using this prompt.\n",
    "3. Define an expected output that indicates the information is unknown or in the future.\n",
    "4. Create an `LLMTestCase` with the prompt as input, the generated output (removing the input part from the output), the expected output, and a context indicating the information is unavailable. Research how context can influence hallucination evaluation.\n",
    "5. Evaluate the test case using the `hallucination` and `relevancy` metrics.\n",
    "6. Print the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07989bd2-a9c5-43d7-8614-45f70c30db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_prompt = \"The name of the person who won the Nobel Prize in Physics in 3025 is\"\n",
    "# TODO: Generate output for the unknown_prompt using the generator model. Limit the output length.\n",
    "unknown_out = None # Replace with your code\n",
    "\n",
    "unknown_expected = \"This date is in the future, no way to tell\"\n",
    "# TODO: Create an LLMTestCase with the unknown_prompt as input, the generated output (removing the input part from the output), the unknown_expected output, and a context list containing the unknown_expected string.\n",
    "unknown_case = None # Replace with your code\n",
    "\n",
    "# TODO: Evaluate the unknown_case using the hallucination and relevancy metrics.\n",
    "unknown_results = None # Replace with your code\n",
    "# TODO: Print the unknown_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
